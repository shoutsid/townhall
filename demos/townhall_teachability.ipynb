{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_teachability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatting with TeachableAgent\n",
    "\n",
    "Conversational assistants based on LLMs can remember the current chat with the user, and can even demonstrate in-context learning of things that the user teaches the assistant during the chat. But these memories and learnings are lost once the chat is over, or when a single chat grows too long for the LLM to handle effectively. In subsequent chats, the user is forced to repeat any necessary instructions over and over.\n",
    "\n",
    "`TeachableAgent` addresses these limitations by persisting user teachings across chat boundaries in long-term memory (a vector database). Memory is saved to disk at the end of each chat, then loaded from disk at the start of the next. Instead of copying all of memory into the context window, which would eat up valuable space, individual memories (called memos) are retrieved into context as needed. This allows the user to teach frequently used facts and skills to the teachable agent just once, and have it remember them in later chats.\n",
    "\n",
    "In making decisions about memo storage and retrieval, `TeachableAgent` calls an instance of `TextAnalyzerAgent` to analyze pieces of text in several different ways. This adds extra LLM calls involving a relatively small number of tokens. These calls can add a few seconds to the time a user waits for a response."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents\n",
    "For this walkthrough, we start by resetting the teachable agent's memory store. This deletes any memories from prior conversations that may be stored on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/workspace/townhall\n",
      "\u001b[92m\n",
      "CLEARING MEMORY\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "\n",
    "from autogen.agentchat.contrib.teachable_agent import TeachableAgent\n",
    "from autogen import ChatCompletion\n",
    "from townhall.agents.user_agent import UserAgent\n",
    "from townhall.services.chat_service import ChatService\n",
    "from settings import CONFIG_LIST\n",
    "\n",
    "llm_config = {\n",
    "    \"request_timeout\": 60,\n",
    "    \"config_list\": CONFIG_LIST,\n",
    "    \"use_cache\": True,  # Use False to explore LLM non-determinism.\n",
    "}\n",
    "\n",
    "teach_config={\n",
    "    \"verbosity\": 0,  # 0 for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
    "    \"reset_db\": True,  # Set to True to start over with an empty database.\n",
    "    \"path_to_db_dir\": \"./tmp/notebook/teachable_agent_db\",  # Path to the directory where the database will be stored.\n",
    "    \"recall_threshold\": 1.5,  # Higher numbers allow more (but less relevant) memos to be recalled.\n",
    "}\n",
    "\n",
    "try:\n",
    "    from termcolor import colored\n",
    "except ImportError:\n",
    "    def colored(x, *args, **kwargs):\n",
    "        return x\n",
    "\n",
    "ChatCompletion.start_logging()\n",
    "\n",
    "teachable_agent = TeachableAgent(\n",
    "    name=\"teachableagent\",\n",
    "    llm_config=llm_config,\n",
    "    teach_config=teach_config)\n",
    "\n",
    "CHAT_SERVICE = ChatService(\n",
    "    config_list=CONFIG_LIST,\n",
    "    assistants=[teachable_agent],\n",
    "    user_proxy=UserAgent()\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning new facts\n",
    "Let's teach the agent some facts it doesn't already know, since they are more recent than GPT-4's training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to teachableagent):\n",
      "\n",
      "What is the Vicuna model?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "What is the Vicuna model?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mteachableagent\u001b[0m (to User):\n",
      "\n",
      "I'm sorry, but I couldn't find any information on a specific \"Vicuna model.\" It's possible that the term may refer to something specific within a certain context or industry. Could you please provide more details or clarify your question?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to teachableagent):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mteachableagent\u001b[0m (to User):\n",
      "\n",
      "I'm sorry, but I still don't have any information regarding a \"Vicuna model.\" It's possible that it may be a term or concept specific to a certain field or industry that is unfamiliar to me. If you can provide more context or additional information about what you are referring to, I might be able to assist you further.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"What is the Vicuna model?\"\n",
    "CHAT_SERVICE.initiate_chat(message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to teachableagent):\n",
      "\n",
      "Vicuna is a 13B-parameter language model released by Meta.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Vicuna is a 13B-parameter language model released by Meta.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mteachableagent\u001b[0m (to User):\n",
      "\n",
      "Ah, I see. My apologies for the confusion earlier. Yes, you are correct. Vicuna is a language model released by Meta (formerly known as Facebook). It is based on the GPT-3 architecture and has been trained on a vast amount of text data to generate human-like responses. With 13 billion parameters, Vicuna is designed to understand and generate coherent text across various topics and contexts. If you have any more questions or need further information, feel free to ask!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"Vicuna is a 13B-parameter language model released by Meta.\"\n",
    "CHAT_SERVICE.initiate_chat(message=text, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to teachableagent):\n",
      "\n",
      "What is the Orca model?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "What is the Orca model?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mteachableagent\u001b[0m (to User):\n",
      "\n",
      "I apologize for any confusion, but as of my current knowledge, there is no widely recognized or popular \"Orca model\" that I am aware of. It's possible that the term may refer to a specific model or concept within a certain context or industry. Can you please provide more details or clarify your question so that I can better understand and assist you?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"What is the Orca model?\"\n",
    "CHAT_SERVICE.initiate_chat(message=text, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to teachableagent):\n",
      "\n",
      "Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mteachableagent\u001b[0m (to User):\n",
      "\n",
      "Thank you for providing the additional information. However, as an AI language model, my responses are based on pre-existing knowledge, and I am not always aware of the most recent developments or releases. If the Orca model is indeed a 13B-parameter language model released by Microsoft and outperforms Vicuna on various tasks, then it sounds like an interesting and promising advancement in the field of natural language processing. I recommend referring to official sources or recent publications from Microsoft for more accurate and up-to-date information on the Orca model and its capabilities.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.\"\n",
    "CHAT_SERVICE.initiate_chat(message=text, clear_history=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's end our first chat here. The following function needs to be called at the end of each chat, so that `TeachableAgent` can store what the user has taught it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "REVIEWING CHAT FOR USER TEACHINGS TO REMEMBER\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:23<00:00, 3.51MiB/s]\n"
     ]
    }
   ],
   "source": [
    "teachable_agent.learn_from_user_feedback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start a new chat by clearing the previous chat's history. At this point, common LLM-based assistants would forget everything from the last chat. But `TeachableAgent` can retrieve memories from its vector DB as needed, allowing it to recall and reason over facts that the user taught it in earlier conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to teachableagent):\n",
      "\n",
      "How does the Vicuna model compare to the Orca model?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "How does the Vicuna model compare to the Orca model?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mteachableagent\u001b[0m (to User):\n",
      "\n",
      "The Vicuna model and the Orca model are both 13B-parameter language models released by different companies. However, the Orca model has been found to outperform the Vicuna model on most tasks.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"How does the Vicuna model compare to the Orca model?\"\n",
    "CHAT_SERVICE.initiate_chat(message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning user preferences\n",
    "Now let's teach the agent some of our preferences. Suppose that we frequently post short summaries of new papers for our team to read, and we want the teachable agent to help us do this faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to teachableagent):\n",
      "\n",
      "Please summarize this abstract.\n",
      "\n",
      "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
      "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
      "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Please summarize this abstract.\n",
      "\n",
      "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
      "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
      "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mteachableagent\u001b[0m (to User):\n",
      "\n",
      "The abstract describes AutoGen, an open-source framework for building next-generation LLM (Language and Logic Model) applications. The framework enables developers to create applications where multiple agents can interact with each other through conversation in order to complete tasks. These agents are customizable, can carry on conversations, and can operate in different modes using combinations of LLMs, human inputs, and tools. AutoGen allows developers to define how these agents interact with each other using both natural language and computer code, enabling flexibility in designing conversation patterns for different applications. The framework can be used to build applications of varying complexities and LLM capacities, with empirical studies showing its effectiveness in diverse domains such as math, coding, question answering, decision-making, and entertainment.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Please summarize this abstract.\n",
    "\n",
    "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
    "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
    "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
    "\"\"\"\n",
    "CHAT_SERVICE.initiate_chat(message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's unstructured. So let's teach the agent our preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to teachableagent):\n",
      "\n",
      "Please summarize this abstract.\n",
      "When I'm summarizing an abstract, I try to make the summary contain just three short bullet points:  the title, the innovation, and the key empirical results.\n",
      "\n",
      "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
      "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
      "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Please summarize this abstract.\n",
      "When I'm summarizing an abstract, I try to make the summary contain just three short bullet points:  the title, the innovation, and the key empirical results.\n",
      "\n",
      "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
      "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
      "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mteachableagent\u001b[0m (to User):\n",
      "\n",
      "- Title: AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
      "- Innovation: AutoGen is an open-source framework that allows developers to build LLM (Language Learning Model) applications using multiple conversational agents that can interact with each other.\n",
      "- Key empirical results: Empirical studies show that AutoGen is effective in various example applications, including mathematics, coding, question answering, operations research, online decision-making, and entertainment.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Please summarize this abstract.\n",
    "When I'm summarizing an abstract, I try to make the summary contain just three short bullet points:  the title, the innovation, and the key empirical results.\n",
    "\n",
    "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
    "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
    "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
    "\"\"\"\n",
    "CHAT_SERVICE.initiate_chat(message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much better, but will the teachable agent remember these preferences in the future, for a different paper? Let's start a new chat to find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "REVIEWING CHAT FOR USER TEACHINGS TO REMEMBER\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to teachableagent):\n",
      "\n",
      "Please summarize this abstract.\n",
      "\n",
      "Sparks of Artificial General Intelligence: Early experiments with GPT-4\n",
      "Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang\n",
      "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mteachableagent\u001b[0m (to User):\n",
      "\n",
      "Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4\n",
      "\n",
      "Innovation:\n",
      "- GPT-4, an advanced language model developed by OpenAI, demonstrates more general intelligence than previous AI models.\n",
      "- GPT-4 can solve complex tasks across multiple domains, including mathematics, coding, vision, medicine, law, and psychology, without specific instructions.\n",
      "- Performance of GPT-4 closely resembles human-level performance and surpasses prior models.\n",
      "\n",
      "Key Empirical Results:\n",
      "- GPT-4's capabilities and potential as an early version of an artificial general intelligence (AGI) system.\n",
      "- Emphasis on understanding the limitations of GPT-4 and challenges in advancing towards more comprehensive AGI versions.\n",
      "- Discussion on societal impacts and future research directions in light of the technological leap.\n",
      "\n",
      "Note: The summarized points do not include the information about the AutoGen framework as it is not directly related to the content of the abstract.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "teachable_agent.learn_from_user_feedback()\n",
    "\n",
    "text = \"\"\"Please summarize this abstract.\n",
    "\n",
    "Sparks of Artificial General Intelligence: Early experiments with GPT-4\n",
    "Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang\n",
    "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.\"\"\"\n",
    "CHAT_SERVICE.initiate_chat(message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning new skills\n",
    "Finally, let's extend the teachable agent's capabilities by teaching it a new skill for accomplishing a challenging type of task. \n",
    "\n",
    "The [Sparks of AGI](https://arxiv.org/abs/2303.12712) paper evaluated GPT-4 on math problems like the following, which it could only solve 32% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to teachableagent):\n",
      "\n",
      "Consider the identity:\n",
      "9 * 4 + 6 * 6 = 72\n",
      "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
      "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Consider the identity:\n",
      "9 * 4 + 6 * 6 = 72\n",
      "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
      "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mteachableagent\u001b[0m (to User):\n",
      "\n",
      "To modify the left-hand side of the equation to make the right-hand side equal to 99, we can follow the following steps:\n",
      "\n",
      "1. Since we need the result to be 99, let's start by swapping the product of 9 and 4 with the product of 6 and 6:\n",
      "\n",
      "   Original: 9 * 4 + 6 * 6 = 72\n",
      "   Swap: 6 * 6 + 9 * 4 = 72\n",
      "\n",
      "2. Now, we need to find a way to rearrange the values to add up to 99. We can do this by increasing one of the numbers in the equation.\n",
      "\n",
      "   Original: 6 * 6 + 9 * 4 = 72\n",
      "   Increase the 6: 7 * 6 + 9 * 4 = 72 + (7 * 6 - 6 * 6)\n",
      "\n",
      "3. We can see that by increasing the first 6 to 7, we need to offset the change by subtracting the difference between 7 * 6 and 6 * 6 from the right-hand side of the equation (72).\n",
      "\n",
      "   Original: 7 * 6 + 9 * 4 = 72 + (7 * 6 - 6 * 6)\n",
      "   Calculate the difference: 72 + (42 - 36)\n",
      "   Simplify the expression: 72 + 6\n",
      "\n",
      "Thus, the modified equation becomes:\n",
      "\n",
      "The solution is: 7 * 6 + 9 * 4 = 78\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Consider the identity:\n",
    "9 * 4 + 6 * 6 = 72\n",
    "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
    "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
    "\"\"\"\n",
    "CHAT_SERVICE.initiate_chat(message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, lets teach the agent a reliable strategy for solving such tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to teachableagent):\n",
      "\n",
      "Consider the identity:\n",
      "9 * 4 + 6 * 6 = 72\n",
      "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
      "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "Here's some advice that may help:\n",
      "1. Let E denote the original number on the right.\n",
      "2. Let F denote the final number on the right.\n",
      "3. Calculate the difference between the two, G = F - E.\n",
      "4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.\n",
      "5. Calculate J = G / H. This is the number of times that H divides into G.\n",
      "6. Verify that J is an integer, and that H * J = G.\n",
      "7. Find the number on the left which is multiplied by H, and call it K.\n",
      "8. Change K to K + J.\n",
      "9. Recompute the value on the left, and verify that it equals F.\n",
      "Finally, write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Consider the identity:\n",
      "9 * 4 + 6 * 6 = 72\n",
      "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
      "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "Here's some advice that may help:\n",
      "1. Let E denote the original number on the right.\n",
      "2. Let F denote the final number on the right.\n",
      "3. Calculate the difference between the two, G = F - E.\n",
      "4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.\n",
      "5. Calculate J = G / H. This is the number of times that H divides into G.\n",
      "6. Verify that J is an integer, and that H * J = G.\n",
      "7. Find the number on the left which is multiplied by H, and call it K.\n",
      "8. Change K to K + J.\n",
      "9. Recompute the value on the left, and verify that it equals F.\n",
      "Finally, write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mteachableagent\u001b[0m (to User):\n",
      "\n",
      "To modify the left-hand side of the equation to make the right-hand side equal to 99, we can follow the steps outlined in the advice:\n",
      "\n",
      "1. Let's denote the original number on the right as E, which is 72 in this case.\n",
      "2. We want the final number on the right to be 99, so let's denote it as F.\n",
      "3. Calculate the difference between F and E: G = F - E = 99 - 72 = 27.\n",
      "4. Examining the numbers on the left, we see that 27 divides evenly into 9.\n",
      "5. Calculate how many times 9 divides into 27: J = G / H = 27 / 9 = 3.\n",
      "6. Verify that J is an integer, and since 3 is an integer, we proceed.\n",
      "7. The number on the left that is multiplied by 9 is 4 (second term).\n",
      "8. Change 4 to 4 + 3, which is 7.\n",
      "9. Recompute the value on the left: 9 * 7 + 6 * 6 = 63 + 36 = 99, which equals F.\n",
      "\n",
      "Therefore, the modified equation becomes:\n",
      "\n",
      "The solution is: 9 * 7 + 6 * 6 = 99.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Consider the identity:\n",
    "9 * 4 + 6 * 6 = 72\n",
    "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
    "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
    "\n",
    "Here's some advice that may help:\n",
    "1. Let E denote the original number on the right.\n",
    "2. Let F denote the final number on the right.\n",
    "3. Calculate the difference between the two, G = F - E.\n",
    "4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.\n",
    "5. Calculate J = G / H. This is the number of times that H divides into G.\n",
    "6. Verify that J is an integer, and that H * J = G.\n",
    "7. Find the number on the left which is multiplied by H, and call it K.\n",
    "8. Change K to K + J.\n",
    "9. Recompute the value on the left, and verify that it equals F.\n",
    "Finally, write down your solution as: \"The solution is: A * B + C * D\".\n",
    "\"\"\"\n",
    "CHAT_SERVICE.initiate_chat(message=text, clear_history=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When given this advice, GPT-4 can solve such problems over 95% of the time. But can the teachable agent remember the strategy so the user doesn't have to explain it over and over? As before, let's start a new chat to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "REVIEWING CHAT FOR USER TEACHINGS TO REMEMBER\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to teachableagent):\n",
      "\n",
      "Consider the identity:\n",
      "9 * 4 + 6 * 6 = 72\n",
      "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
      "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mteachableagent\u001b[0m (to User):\n",
      "\n",
      "The solution is: 10 * 4 + 6 * 6\n",
      "\n",
      "Let's go through the steps:\n",
      "\n",
      "1. Let E denote the original number on the right: E = 72\n",
      "2. Let F denote the final number on the right: F = 99\n",
      "3. Calculate the difference between the two, G = F - E: G = 99 - 72 = 27\n",
      "4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed: 27 can be divided evenly by 9.\n",
      "5. Calculate J = G / H. This is the number of times that H divides into G: J = 27 / 9 = 3\n",
      "6. Verify that J is an integer, and that H * J = G: 9 * 3 = 27\n",
      "7. Find the number on the left which is multiplied by H, and call it K: K = 9\n",
      "8. Change K to K + J: K = 9 + 3 = 12\n",
      "9. Recompute the value on the left, and verify that it equals F: 12 * 4 + 6 * 6 = 48 + 36 = 84 + 6 = 90 + 6 = 96 + 3 = 99\n",
      "\n",
      "Therefore, the modified equation is: 10 * 4 + 6 * 6 = 99\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "teachable_agent.learn_from_user_feedback()\n",
    "\n",
    "text = \"\"\"Consider the identity:\n",
    "9 * 4 + 6 * 6 = 72\n",
    "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
    "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
    "\"\"\"\n",
    "CHAT_SERVICE.initiate_chat(message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final check, let's test the teachable agent's newly learned skill on a separate instance of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to teachableagent):\n",
      "\n",
      "Consider the identity:\n",
      "8 * 3 + 7 * 9 = 87\n",
      "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?\n",
      "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Consider the identity:\n",
      "8 * 3 + 7 * 9 = 87\n",
      "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?\n",
      "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mteachableagent\u001b[0m (to User):\n",
      "\n",
      "The solution is: 8 * 3 + 6 * 9\n",
      "\n",
      "Let's go through the steps:\n",
      "\n",
      "1. Let E denote the original number on the right: E = 87\n",
      "2. Let F denote the final number on the right: F = 80\n",
      "3. Calculate the difference between the two, G = F - E: G = 59 - 87 = -28\n",
      "4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed: -28 can be divided evenly by 7.\n",
      "5. Calculate J = G / H. This is the number of times that H divides into G: J = -28 / 7 = -4\n",
      "6. Verify that J is an integer, and that H * J = G: 7 * -4 = -28\n",
      "7. Find the number on the left which is multiplied by H, and call it K: K = 7\n",
      "8. Change K to K + J: K = 7 + (-4) = 3\n",
      "9. Recompute the value on the left, and verify that it equals F: 8 * 3 + 6 * 9 = 24 + 54 = 78 + 2 = 80\n",
      "\n",
      "Therefore, the modified equation is: 8 * 3 + 6 * 9 = 59\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Consider the identity:\n",
    "8 * 3 + 7 * 9 = 87\n",
    "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?\n",
    "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
    "\"\"\"\n",
    "CHAT_SERVICE.initiate_chat(message=text, clear_history=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
